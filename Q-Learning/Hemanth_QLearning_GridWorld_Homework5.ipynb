{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d855a3",
   "metadata": {},
   "source": [
    "# This code is written by Hemanth Chebiyam.\n",
    "## Email: hc3746@rit.edu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "80a3e4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import operator\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f039efa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, Ny=8, Nx=8):\n",
    "        # State space\n",
    "        self.Ny = Ny\n",
    "        self.Nx = Nx\n",
    "        self.state_dim = (Ny, Nx)\n",
    "        \n",
    "        # Action space\n",
    "        self.action_dim = (4,)\n",
    "        self.action_dict = {\"UP\": 0, \"RIGHT\": 1, \"DOWN\": 2, \"LEFT\": 3}\n",
    "        self.action_coords = [(-1, 0), (0, 1), (1, 0), (0, -1)]\n",
    "        \n",
    "        # Rewards\n",
    "        self.R = self._build_rewards()\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset agent state\n",
    "        self.state = (0, 0)\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        # Evolve agent state\n",
    "        state_next = (self.state[0] + self.action_coords[action][0],\n",
    "                      self.state[1] + self.action_coords[action][1])\n",
    "        # Collect reward\n",
    "        reward = self.R[self.state + (action,)]\n",
    "        # Terminate at goal state (bottom-right grid corner)\n",
    "        done = (state_next[0] == self.Ny - 1) and (state_next[1] == self.Nx - 1)\n",
    "        # Update state\n",
    "        self.state = state_next\n",
    "        return state_next, reward, done\n",
    "\n",
    "    def allowed_actions(self):\n",
    "        # List of actions allowed\n",
    "        actions_allowed = []\n",
    "        # Based on agent grid location\n",
    "        y, x = self.state[0], self.state[1]\n",
    "        if y > 0: # top-boundary\n",
    "            actions_allowed.append(self.action_dict[\"UP\"])\n",
    "        if y < self.Ny - 1: # bottom-boundary\n",
    "            actions_allowed.append(self.action_dict[\"DOWN\"])\n",
    "        if x > 0: # left-boundary\n",
    "            actions_allowed.append(self.action_dict[\"LEFT\"])\n",
    "        if x < self.Nx - 1: # right-boundary\n",
    "            actions_allowed.append(self.action_dict[\"RIGHT\"])\n",
    "        actions_allowed = np.array(actions_allowed, dtype=int)\n",
    "        return actions_allowed\n",
    "\n",
    "    def _build_rewards(self):\n",
    "        # agent rewards R[s,a]\n",
    "        r_goal = 100 # reward for arriving at goal state (bottom-right corner)\n",
    "        r_nongoal = -0.1 # penalty for not reaching goal state\n",
    "        R = r_nongoal * np.ones(self.state_dim + self.action_dim, dtype=float)\n",
    "        R[self.Ny - 2, self.Nx - 1, self.action_dict[\"DOWN\"]] = r_goal # arrive from above\n",
    "        R[self.Ny - 1, self.Nx - 2, self.action_dict[\"RIGHT\"]] = r_goal # arrive from the left\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c1c1da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        # state and action dimension\n",
    "        self.state_dim = env.state_dim\n",
    "        self.action_dim = env.action_dim\n",
    "        # Agent learning parameters\n",
    "        self.epsilon = 1.0 # initial exploration probability\n",
    "        self.epsilon_decay = 0.99 # epsilon decay after each episode\n",
    "        self.alpha = 0.1 # learning rate\n",
    "        self.gamma = 0.9 # reward discount factor\n",
    "        # Initializing Q[s,a] table\n",
    "        self.Q = np.zeros(self.state_dim + self.action_dim, dtype=float)\n",
    "\n",
    "    def get_action(self, env):\n",
    "        # Epsilon-greedy agent policy\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            # explore\n",
    "            return np.random.choice(env.allowed_actions())\n",
    "        else:\n",
    "            # exploit on allowed actions\n",
    "            state = env.state\n",
    "            actions_allowed = env.allowed_actions()\n",
    "            Q_s = self.Q[state[0], state[1], actions_allowed]\n",
    "            actions_greedy = actions_allowed[np.flatnonzero(Q_s == np.max(Q_s))]\n",
    "            return np.random.choice(actions_greedy)\n",
    "\n",
    "    def train(self, memory):\n",
    "\n",
    "        # Update:\n",
    "        # Q[s,a] <- (1 - alpha) * Q[s,a] + alpha * (R[s,a] + gamma * max(Q[s',:]))\n",
    "        # R[s,a] = reward for taking action a from state s\n",
    "        (state, action, state_next, reward, done) = memory\n",
    "        sa = state + (action,)\n",
    "        self.Q[sa] = (1 - self.alpha) * self.Q[sa] + self.alpha * (reward + self.gamma * np.max(self.Q[state_next]))\n",
    "\n",
    "    def display_greedy_policy(self):\n",
    "        # greedy policy\n",
    "        greedy_policy = np.zeros((self.state_dim[0], self.state_dim[1]), dtype=int)\n",
    "        for x in range(self.state_dim[0]):\n",
    "            for y in range(self.state_dim[1]):\n",
    "                greedy_policy[y, x] = np.argmax(self.Q[y, x, :])\n",
    "        print(\"\\nGreedy policy(y, x):\")\n",
    "        print(greedy_policy)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1eb37b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Brain:\n",
    "    def __init__(self, env, agent):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "\n",
    "    def train_agent(self, N_episodes=500):\n",
    "        # Train agent\n",
    "        print(\"\\nTraining agent...\\n\")\n",
    "        for episode in range(N_episodes):\n",
    "            # Generate an episode\n",
    "            iter_episode, reward_episode = 0, 0\n",
    "            state = self.env.reset() # starting state\n",
    "            while True:\n",
    "                action = self.agent.get_action(self.env)\n",
    "                state_next, reward, done = self.env.step(action) # evolve state by action\n",
    "                self.agent.train((state, action, state_next, reward, done)) # train agent\n",
    "                iter_episode += 1\n",
    "                reward_episode += reward\n",
    "                if done:\n",
    "                    break\n",
    "                state = state_next # transition to next state\n",
    "            \n",
    "            # Decay agent exploration parameter\n",
    "            self.agent.epsilon = max(self.agent.epsilon * self.agent.epsilon_decay, 0.01)\n",
    "\n",
    "            if (episode == 0) or (episode + 1) % 10 == 0:\n",
    "                print(\"[episode {}/{}] eps = {:.3F} -> iter = {}, rew = {:.1F}\".format(\n",
    "                    episode + 1, N_episodes, self.agent.epsilon, iter_episode, reward_episode))\n",
    "\n",
    "            if episode == N_episodes - 1:\n",
    "                self.agent.display_greedy_policy()\n",
    "                for (key, val) in sorted(env.action_dict.items(), key=operator.itemgetter(1)):\n",
    "                    print(\" action['{}'] = {}\".format(key, val))\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1fbf386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training agent...\n",
      "\n",
      "[episode 1/500] eps = 0.990 -> iter = 126, rew = 87.5\n",
      "[episode 10/500] eps = 0.904 -> iter = 240, rew = 76.1\n",
      "[episode 20/500] eps = 0.818 -> iter = 64, rew = 93.7\n",
      "[episode 30/500] eps = 0.740 -> iter = 94, rew = 90.7\n",
      "[episode 40/500] eps = 0.669 -> iter = 56, rew = 94.5\n",
      "[episode 50/500] eps = 0.605 -> iter = 22, rew = 97.9\n",
      "[episode 60/500] eps = 0.547 -> iter = 32, rew = 96.9\n",
      "[episode 70/500] eps = 0.495 -> iter = 30, rew = 97.1\n",
      "[episode 80/500] eps = 0.448 -> iter = 26, rew = 97.5\n",
      "[episode 90/500] eps = 0.405 -> iter = 12, rew = 98.9\n",
      "[episode 100/500] eps = 0.366 -> iter = 28, rew = 97.3\n",
      "[episode 110/500] eps = 0.331 -> iter = 28, rew = 97.3\n",
      "[episode 120/500] eps = 0.299 -> iter = 12, rew = 98.9\n",
      "[episode 130/500] eps = 0.271 -> iter = 14, rew = 98.7\n",
      "[episode 140/500] eps = 0.245 -> iter = 16, rew = 98.5\n",
      "[episode 150/500] eps = 0.221 -> iter = 16, rew = 98.5\n",
      "[episode 160/500] eps = 0.200 -> iter = 14, rew = 98.7\n",
      "[episode 170/500] eps = 0.181 -> iter = 16, rew = 98.5\n",
      "[episode 180/500] eps = 0.164 -> iter = 12, rew = 98.9\n",
      "[episode 190/500] eps = 0.148 -> iter = 12, rew = 98.9\n",
      "[episode 200/500] eps = 0.134 -> iter = 14, rew = 98.7\n",
      "[episode 210/500] eps = 0.121 -> iter = 14, rew = 98.7\n",
      "[episode 220/500] eps = 0.110 -> iter = 12, rew = 98.9\n",
      "[episode 230/500] eps = 0.099 -> iter = 12, rew = 98.9\n",
      "[episode 240/500] eps = 0.090 -> iter = 12, rew = 98.9\n",
      "[episode 250/500] eps = 0.081 -> iter = 12, rew = 98.9\n",
      "[episode 260/500] eps = 0.073 -> iter = 12, rew = 98.9\n",
      "[episode 270/500] eps = 0.066 -> iter = 12, rew = 98.9\n",
      "[episode 280/500] eps = 0.060 -> iter = 12, rew = 98.9\n",
      "[episode 290/500] eps = 0.054 -> iter = 12, rew = 98.9\n",
      "[episode 300/500] eps = 0.049 -> iter = 12, rew = 98.9\n",
      "[episode 310/500] eps = 0.044 -> iter = 12, rew = 98.9\n",
      "[episode 320/500] eps = 0.040 -> iter = 12, rew = 98.9\n",
      "[episode 330/500] eps = 0.036 -> iter = 12, rew = 98.9\n",
      "[episode 340/500] eps = 0.033 -> iter = 12, rew = 98.9\n",
      "[episode 350/500] eps = 0.030 -> iter = 12, rew = 98.9\n",
      "[episode 360/500] eps = 0.027 -> iter = 14, rew = 98.7\n",
      "[episode 370/500] eps = 0.024 -> iter = 14, rew = 98.7\n",
      "[episode 380/500] eps = 0.022 -> iter = 14, rew = 98.7\n",
      "[episode 390/500] eps = 0.020 -> iter = 12, rew = 98.9\n",
      "[episode 400/500] eps = 0.018 -> iter = 12, rew = 98.9\n",
      "[episode 410/500] eps = 0.016 -> iter = 12, rew = 98.9\n",
      "[episode 420/500] eps = 0.015 -> iter = 12, rew = 98.9\n",
      "[episode 430/500] eps = 0.013 -> iter = 12, rew = 98.9\n",
      "[episode 440/500] eps = 0.012 -> iter = 12, rew = 98.9\n",
      "[episode 450/500] eps = 0.011 -> iter = 12, rew = 98.9\n",
      "[episode 460/500] eps = 0.010 -> iter = 12, rew = 98.9\n",
      "[episode 470/500] eps = 0.010 -> iter = 12, rew = 98.9\n",
      "[episode 480/500] eps = 0.010 -> iter = 12, rew = 98.9\n",
      "[episode 490/500] eps = 0.010 -> iter = 12, rew = 98.9\n",
      "[episode 500/500] eps = 0.010 -> iter = 12, rew = 98.9\n",
      "\n",
      "Greedy policy(y, x):\n",
      "[[1 1 1 1 2 2 3]\n",
      " [2 0 2 1 2 2 2]\n",
      " [1 1 1 1 2 2 2]\n",
      " [0 2 2 1 1 2 2]\n",
      " [3 1 2 2 2 2 2]\n",
      " [1 1 1 2 1 2 2]\n",
      " [1 1 1 1 1 1 0]]\n",
      "\n",
      " action['UP'] = 0\n",
      " action['RIGHT'] = 1\n",
      " action['DOWN'] = 2\n",
      " action['LEFT'] = 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "env = Environment(Ny=7, Nx=7)\n",
    "agent = Agent(env)\n",
    "brain = Brain(env, agent)\n",
    "\n",
    "# Train agent\n",
    "brain.train_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
